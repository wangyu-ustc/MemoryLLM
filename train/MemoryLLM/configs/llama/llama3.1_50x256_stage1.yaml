model:
  base_learning_rate: 4.6e-5
  target: MemoryLLM.memoryllm.models.memory.LlamaMemoryModelPL
  params:
    monitor: val/total_loss
    num_blocks: 50
    num_tokens: 256
    update_memory_during_training: true
    max_seq_length: 2048
    initialized: true
    lr_scheduler:
      target: torch.optim.lr_scheduler.LinearLR
      params:
        start_factor: 1
        end_factor: 0.1
        total_iters: 100000
    lora_config:
      inference_mode: false
      r: 16
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules: ['q_proj', 'v_proj', 'k_proj', 'up_proj', 'down_proj', 'gate_proj']
    module_name: LlamaDropMemoryModel
    model_path: meta-llama/Meta-Llama-3.1-8B
    # gradient_checkpointing: false
    add_decoder_lora: true
    # initialize_decoder_lora_from_default: true
    add_bos_embedding: true
    cat_and_drop_memory: true
    cache_data_for_longer_context: true
    new_memory_embedding_fullset: true
    shrink_to_one_embedding: true
    # cat_to_maximum_memory: true
    occassionally_cat_to_maximum_memory_ratio: 0.8
    # one_context_ratio: 0.5
    keep_gradient_for_the_last_step: false
    attn_implementation: flash_attention_2
    drop_memory_per_layer: true
    # half_last: true
    # num_of_additional_tokens_to_mask: 5
    full_context_and_sentence_training_ratio: 0.1
    # rope_scaling: 
    #   type: linear
    instruct: "Repeat:"
    mask_instruction_tokens: true
    num_of_additional_tokens_to_mask: 5
    gradient_checkpointing: false
    efficient_get_delta_memory: true
    num_contexts_schedule:
      checkpoints: [100000, 200000]
      values: [50, 100, 200]
    random_sample_length_ratio: 0.0
    pass_ratio: 0.8
    empty_prob: 1.0
    # detach_memory_ratio: 0.2
    validation_dataset_names:
      # pretraining dataset
      - fineweb
      # long context qa
      - narrativeqa
      - 2wikimqa
      - hotpotqa
      - qasper
      - musique
      - multifieldqa_en

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 1
    eval_batch_size: 1
    eval_max_length: 462
    num_workers: 8
    num_tokens: 256
    wrap: False
    train:
      target: MemoryLLM.memoryllm.data.fineweb.FineWebEduDataset
      params:
        root: HuggingFaceFW/fineweb-edu
        name: CC-MAIN-2024-10
        split: train
        tokenizer_path: meta-llama/Meta-Llama-3.1-8B
        max_length: 512
        min_length: 16
        max_seq_length: 2048
        add_special_tokens: false
        target_is_context_ratio: 0.1
        start: 0
        end: 3000000

    validation:
      - target: MemoryLLM.memoryllm.data.fineweb.FineWebEduValDataset
        params:
          root: HuggingFaceFW/fineweb-edu
          name: CC-MAIN-2024-10
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_seq_length: 2048
          num: 1000
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: narrativeqa
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          max_chunk_length: 2048
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: 2wikimqa
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          max_chunk_length: 2048
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: hotpotqa
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          max_chunk_length: 2048
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: qasper
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          max_chunk_length: 2048
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: musique
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          max_chunk_length: 2048
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: multifieldqa_en
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          max_chunk_length: 2048
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json

lightning:
  trainer:
    accelerator: gpu
    strategy: deepspeed_stage_2
    # limit_train_batches: 10000 # this is lethal, as it will limit 10000 batches and only train on that 10000 batches over and over again
    # every_n_train_steps: 10
    precision: bf16-mixed
    val_check_interval: 10000
    # accumulate_grad_batches: 4