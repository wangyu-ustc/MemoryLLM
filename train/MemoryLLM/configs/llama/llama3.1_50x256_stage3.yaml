model:
  base_learning_rate: 4.6e-6
  target: MemoryLLM.memoryllm.models.memory.LlamaMemoryModelPL
  params:
    monitor: val/total_loss
    num_blocks: 50
    num_tokens: 256
    update_memory_during_training: true
    max_seq_length: 2048
    lora_config:
      inference_mode: false
      r: 16
      lora_alpha: 32
      lora_dropout: 0.1
      target_modules: ['q_proj', 'v_proj', 'k_proj', 'up_proj', 'down_proj', 'gate_proj']
    module_name: LlamaDropMemoryLTMModel
    model_path: meta-llama/Meta-Llama-3.1-8B
    add_decoder_lora: true
    add_bos_embedding: true
    cat_and_drop_memory: true
    cache_data_for_longer_context: true
    new_memory_embedding_fullset: true
    shrink_to_one_embedding: true
    occassionally_cat_to_maximum_memory_ratio: 0.8
    keep_gradient_for_the_last_step: false
    attn_implementation: flash_attention_2
    drop_memory_per_layer: true
    full_context_and_sentence_training_ratio: 0.05
    instruct: "Repeat:"
    gradient_checkpointing: false
    mask_instruction_tokens: true
    num_of_additional_tokens_to_mask: 5
    num_contexts_schedule:
      checkpoints: [20000]
      values: [100, 200]
    random_sample_length_ratio: 0.0
    pass_ratio: 0.8
    empty_prob: 0.2
    validation_dataset_names:
      - naturalqa
      - squad
      # pretraining dataset
      - fineweb
      # long context qa
      - narrativeqa
      - 2wikimqa
      - hotpotqa
      - qasper
      - musique
      - multifieldqa_en
    # LTM configs:
    update_ltm_mode: immediate
    num_ltm_blocks: 10
    update_ltm_frequency: 60
    decay_frequency: 1
    initial_rf_when_moving_stm_to_ltm: 10
    # retriever/selector configs
    add_penalty_on_retriever_weights: true
    retriever_penalty_weight: 0.001
    map_from_hidden_states: true
    add_selector: true
    selector_hidden_dim: 256
    selector_learning_rate: 1e-4
    selector_loss_type: bce
    num_selector_layers: 2
    bce_loss_weight: 1.0
    detach_hidden_state: true
    negative_loss_weight: 1
    random_retriever_length: true
    add_encoder_retriever: true
    use_retriever_during_validation: false
    put_cached_dropped_memory_on_cpu: true
    # reinit_memory: false
    ckpt_path: ./logs/2024-11-19T23-13-38_llama3.1_50x256_long_ltm/checkpoints/last.ckpt

data:
  target: main.DataModuleFromConfig
  params:
    batch_size: 1
    eval_batch_size: 1
    eval_max_length: 462
    num_workers: 2
    num_tokens: 256
    wrap: False
    use_worker_init_fn: true
    worker_init_fn: worker_init_fn_mixed
    train:
      target: MemoryLLM.memoryllm.data.mixedstream.MixedStreamDataset
      params:
        root: HuggingFaceFW/fineweb-edu
        longdoc_root: ./local_data/long_data/
        name: "local_data/fineweb-edu-CC-MAIN-2023-50"
        split: train
        tokenizer_path: meta-llama/Meta-Llama-3.1-8B
        max_length: 512
        min_length: 16
        longdoc_min_length: 384
        max_seq_length: 2048
        add_special_tokens: false
        target_is_context_ratio: 0.1
        long_documents_ratios:
          4k-8k: 0.025
          8k-16k: 0.025
          16k-32k: 0.025
          32k-64k: 0.025
        short_document_start: 720000
        short_document_end: 1440000

    validation:
      - target: MemoryLLM.memoryllm.data.cqa.CQADataset
        params:
          root: ./cqa_data/nqa
          num_unrelated_contexts: 120
          num: 100
      - target: MemoryLLM.memoryllm.data.cqa.CQADataset
        params:
          root: ./cqa_data/squad
          num_unrelated_contexts: 120
          num: 100
      - target: MemoryLLM.memoryllm.data.fineweb.FineWebEduValDataset
        params:
          root: HuggingFaceFW/fineweb-edu
          name: CC-MAIN-2024-10
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_seq_length: 2048
          num: 1000
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: narrativeqa
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: 2wikimqa
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: hotpotqa
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: qasper
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: musique
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json
      - target: MemoryLLM.memoryllm.data.longcontext.LongContextDataset
        params:
          dataset: multifieldqa_en
          tokenizer_path: meta-llama/Meta-Llama-3.1-8B
          max_length: 8192
          prompt_format_path: longbench_config/dataset2prompt_mem.json
          maxlen_path: longbench_config/dataset2maxlen.json

lightning:
  trainer:
    accelerator: gpu
    strategy: deepspeed_stage_2
    precision: bf16-mixed
    val_check_interval: 10000
    # accumulate_grad_batches: 4
